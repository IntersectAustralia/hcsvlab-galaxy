<tool id="Tokenize" name="Tokenize Text" version="0.01">
    <description>Split a text into words</description>
    <command interpreter="python">
        g_tokenize.py --input $input1 --output $tokens $lower $nopunct
    </command>
    <requirements>
        <requirement type="set_environment">PATH</requirement>
        <requirement type="set_environment">PYTHONPATH</requirement>
        <requirement type="set_environment">NLTK_DATA</requirement>
        <requirement type="package" version="1.0">python_nltk_tools</requirement>
    </requirements>
    <inputs>
        <param name="input1" type="data" format="txt"
               label="Select a suitable input file from your history"/>

        <param name='lower' type="boolean" truevalue="--lower" falsevalue=""
               label="Lowercase all tokens?"/>

        <param name='nopunct' type="boolean" truevalue="--nopunct" falsevalue=""
               label="Remove punctuation?"/>
    </inputs>
    <outputs>
        <data format="json" name="tokens" label="${input1.name} Tokens"/>
    </outputs>
    <tests>
        <test>
            <param name='input1' value='sample_text.txt'/>
            <param name='lower' value='--lower'/>
            <param name='nopunct' value='--nopunct'/>
            <output name='tokens' file='sample_text_lower_nopunct.json'/>
        </test>
        <test>
            <param name='input1' value='sample_text.txt'/>
            <param name='lower' value='--lower'/>
            <param name='nopunct' value=''/>
            <output name='tokens' file='sample_text_lower.json'/>
        </test>
        <test>
            <param name='input1' value='sample_text.txt'/>
            <param name='lower' value=''/>
            <param name='nopunct' value=''/>
            <output name='tokens' file='sample_text_tok.json'/>
        </test>

    </tests>

    <help>
        Tokenize a text into separate words, optionally remove punctuation and convert to lower case.
    </help>

</tool>
